{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas for data manipulation\n",
    "#numpy for computaion\n",
    "#Sklearn - for Machine Learning model selection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the new file\n",
    "temp=pd.read_csv('trimmedData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=temp['X']\n",
    "X=X.values.reshape(-1,1)\n",
    "Y=temp['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grouping of data \n",
    "and creating chunks of size window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groupby Y value, (1,2,32,37,38)\n",
    "gtemp=temp.groupby('Y')\n",
    "#initializing y\n",
    "y=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing all on the basis of their label\n",
    "#get_group(1) reutrns the group with y label 1\n",
    "g1=gtemp.get_group(1) \n",
    "g2=gtemp.get_group(2)\n",
    "g32=gtemp.get_group(32)\n",
    "g37=gtemp.get_group(37)\n",
    "g38=gtemp.get_group(38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_features =1 (amplitude)\n",
    "n_features=1\n",
    "#initialise total data point to 0\n",
    "totaldp=0\n",
    "#window size =250 ==1 sec\n",
    "window=250\n",
    "\n",
    "#get amplitude values from X\n",
    "X=g1['X']\n",
    "#number of data points = number of rows in X / window\n",
    "data_points=X.shape[0]//window\n",
    "#appending y \n",
    "y=np.append(y,[1]*data_points)\n",
    "#updating total datapoints\n",
    "totaldp+=data_points\n",
    "#removing left points\n",
    "X=X.head(data_points*window)\n",
    "#converting to array\n",
    "X=np.array(X)\n",
    "#reshaping it to data_points,window,n_features(n,250,1)\n",
    "X=np.reshape(X,((data_points,window,n_features)))\n",
    "#grouped X is the new amplitude chunk\n",
    "groupedX=X\n",
    "\n",
    "#same procedure for label 2\n",
    "X=g2['X']\n",
    "data_points=X.shape[0]//window\n",
    "y=np.append(y,[2]*data_points)\n",
    "totaldp+=data_points\n",
    "X=X.head(data_points*window)\n",
    "X=np.array(X)\n",
    "X=np.reshape(X,((data_points,window,n_features)))\n",
    "groupedX=np.append(groupedX,X)\n",
    "\n",
    "#same procedure for label 32\n",
    "X=g32['X']\n",
    "data_points=X.shape[0]//window\n",
    "y=np.append(y,[32]*data_points)\n",
    "totaldp+=data_points\n",
    "X=X.head(data_points*window)\n",
    "X=np.array(X)\n",
    "X=np.reshape(X,((data_points,window,n_features)))\n",
    "groupedX=np.append(groupedX,X)\n",
    "\n",
    "\n",
    "X=g37['X']\n",
    "data_points=X.shape[0]//window\n",
    "y=np.append(y,[37]*data_points)\n",
    "totaldp+=data_points\n",
    "X=X.head(data_points*window)\n",
    "X=np.array(X)\n",
    "X=np.reshape(X,((data_points,window,n_features)))\n",
    "groupedX=np.append(groupedX,X)\n",
    "\n",
    "\n",
    "X=g38['X']\n",
    "data_points=X.shape[0]//window\n",
    "y=np.append(y,[38]*data_points)\n",
    "totaldp+=data_points\n",
    "X=X.head(data_points*window)\n",
    "X=np.array(X)\n",
    "X=np.reshape(X,((data_points,window,n_features)))\n",
    "groupedX=np.append(groupedX,X)\n",
    "\n",
    "\n",
    "groupedX=np.reshape(groupedX,((totaldp,window,n_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=groupedX\n",
    "#max value of colomn (axis specifis row or colomn)\n",
    "a=np.max(X,axis=1)\n",
    "#concatinate with minimum, second axis is for concatinate into colomn\n",
    "a=np.concatenate((a,np.min(X,axis=1)),axis=1)\n",
    "a=np.concatenate((a,np.average(X,axis=1)),axis=1)\n",
    "#standard deviation\n",
    "a=np.concatenate((a,np.std(X,axis=1)),axis=1)\n",
    "#fft real part\n",
    "a=np.concatenate((a,np.real(np.fft.rfftn(X,axes=(0,1)))[:,0]),axis=1)\n",
    "X=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          1\n",
       "1          1\n",
       "2          1\n",
       "3          1\n",
       "4          1\n",
       "5          1\n",
       "6          1\n",
       "7          1\n",
       "8          1\n",
       "9          1\n",
       "10         1\n",
       "11         1\n",
       "12         1\n",
       "13         1\n",
       "14         1\n",
       "15         1\n",
       "16         1\n",
       "17         1\n",
       "18         1\n",
       "19         1\n",
       "20         1\n",
       "21         1\n",
       "22         1\n",
       "23         1\n",
       "24         1\n",
       "25         1\n",
       "26         1\n",
       "27         1\n",
       "28         1\n",
       "29         1\n",
       "          ..\n",
       "657402    37\n",
       "657403    37\n",
       "657404    37\n",
       "657405    37\n",
       "657406    37\n",
       "657407    37\n",
       "657408    37\n",
       "657409    37\n",
       "657410    37\n",
       "657411    37\n",
       "657412    37\n",
       "657413    37\n",
       "657414    37\n",
       "657415    37\n",
       "657416    37\n",
       "657417    37\n",
       "657418    37\n",
       "657419    37\n",
       "657420    37\n",
       "657421    37\n",
       "657422    37\n",
       "657423    37\n",
       "657424    37\n",
       "657425    37\n",
       "657426    37\n",
       "657427    37\n",
       "657428    37\n",
       "657429    37\n",
       "657430    37\n",
       "657431    37\n",
       "Name: Y, Length: 657432, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(868,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Classes\n",
    "1 new mind wonder           \n",
    "2 new sleepy        \n",
    "32 peek experiences        \n",
    "37 some meditation     \n",
    "38 good meditation      \n",
    "\n",
    "Mind Wonder and aleepy are 5-10 sec after it actually occoured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "clf.score(X_test,y_test)\n",
    "y_pre=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[368  21   0  22   3]\n",
      " [ 22  55   0   9   0]\n",
      " [  0   0   1   0   0]\n",
      " [ 28   5   0 218  16]\n",
      " [  3   0   0   6  91]]\n",
      "Accuracy Score : 0.8444700460829493\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.87      0.89      0.88       414\n",
      "         2.0       0.68      0.64      0.66        86\n",
      "        32.0       1.00      1.00      1.00         1\n",
      "        37.0       0.85      0.82      0.84       267\n",
      "        38.0       0.83      0.91      0.87       100\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       868\n",
      "   macro avg       0.85      0.85      0.85       868\n",
      "weighted avg       0.84      0.84      0.84       868\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Analysing Results - Accuracy, precision, recall, F1-score for each class\n",
    "actual=y_test\n",
    "predicted=y_pre\n",
    "results = confusion_matrix(actual, predicted) \n",
    "print ('Confusion Matrix :')\n",
    "print(results) \n",
    "print ('Accuracy Score :',accuracy_score(actual, predicted) )\n",
    "#print ('Report : ')\n",
    "print (classification_report(actual, predicted) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train,y_train)\n",
    "clf.score(X_test,y_test)\n",
    "y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[381  18   0  15   0]\n",
      " [ 25  61   0   0   0]\n",
      " [  0   0   1   0   0]\n",
      " [ 25   8   0 226   8]\n",
      " [  7   0   0   8  85]]\n",
      "Accuracy Score : 0.868663594470046\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.87      0.92      0.89       414\n",
      "         2.0       0.70      0.71      0.71        86\n",
      "        32.0       1.00      1.00      1.00         1\n",
      "        37.0       0.91      0.85      0.88       267\n",
      "        38.0       0.91      0.85      0.88       100\n",
      "\n",
      "   micro avg       0.87      0.87      0.87       868\n",
      "   macro avg       0.88      0.87      0.87       868\n",
      "weighted avg       0.87      0.87      0.87       868\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actual=y_test\n",
    "predicted=y_pred\n",
    "results = confusion_matrix(actual, predicted) \n",
    "print ('Confusion Matrix :')\n",
    "print(results) \n",
    "print ('Accuracy Score :',accuracy_score(actual, predicted) )\n",
    "#print ('Report : ')\n",
    "print (classification_report(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn=KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train,y_train)\n",
    "knn.score(X_test,y_test)\n",
    "y_pred=knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[262  27   0  83  42]\n",
      " [ 62  18   0   6   0]\n",
      " [  1   0   0   0   0]\n",
      " [213   4   0  48   2]\n",
      " [ 73   0   0   6  21]]\n",
      "Accuracy Score : 0.402073732718894\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.43      0.63      0.51       414\n",
      "         2.0       0.37      0.21      0.27        86\n",
      "        32.0       0.00      0.00      0.00         1\n",
      "        37.0       0.34      0.18      0.23       267\n",
      "        38.0       0.32      0.21      0.25       100\n",
      "\n",
      "   micro avg       0.40      0.40      0.40       868\n",
      "   macro avg       0.29      0.25      0.25       868\n",
      "weighted avg       0.38      0.40      0.37       868\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Babbar\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "actual=y_test\n",
    "predicted=y_pred\n",
    "results = confusion_matrix(actual, predicted) \n",
    "print ('Confusion Matrix :')\n",
    "print(results) \n",
    "print ('Accuracy Score :',accuracy_score(actual, predicted) )\n",
    "#print ('Report : ')\n",
    "print (classification_report(actual, predicted) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of chunks in each classa.      \n",
    "1 -new mind wonder - 1260         \n",
    "b. 2 -new sleepy - 261              \n",
    "c. 32 -peak experience - 8        \n",
    "d. 37 -some meditation - 785      \n",
    "e. 38 -good meditation - 314    \n",
    "2. Training by 67%(train data), Predicting the Test Data results and comparing to actual labels\n",
    "3. Analysing Results - Accuracy, precision, recall, F1-score for each class in Random Forest Classifier with number of estimators =100 here the class with the worst accuracy is 2 - sleepy (Probably due to verysmall training set and high resemblance with class 1 - mind wonder(18 points shifted) this may be due to labeling the same way.\n",
    "4. Best accuracy for class 1 (so if we increase the data we can expect the accuracy to go high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
